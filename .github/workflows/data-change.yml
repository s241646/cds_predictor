name: Data Change Workflow

on:
  push:
    paths:
      - "data.dvc"
      - ".dvc/**"
      - "src/cds_repository/data.py"
      - "configs/**"
  workflow_dispatch:

jobs:
  data-check:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}
      - name: Setup uv
        uses: astral-sh/setup-uv@v7
        with:
          python-version: "3.12"
      - name: Install dependencies
        run: uv sync
      - name: Pull data with DVC
        run: uv run dvc pull --force
      - name: Quick data check
        run: |
          uv run python - <<'PY'
          from pathlib import Path
          from cds_repository.data import get_dataloaders

          def resolve_data_path() -> Path:
              candidates = []
              for train_file in Path("data").rglob("train.csv.gz"):
                  if (train_file.parent / "val.csv.gz").exists() and (train_file.parent / "test.csv.gz").exists():
                      return train_file.parent
                  candidates.append(train_file.parent)
              if candidates:
                  return candidates[0]
              raise FileNotFoundError("No train.csv.gz found under data/")

          data_path = resolve_data_path()
          print(f"Using data path: {data_path}")
          get_dataloaders(data_path=data_path, batch_size=4)
          PY
